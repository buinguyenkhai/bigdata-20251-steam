apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-test-script
data:
  kafka-test-output.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, upper
    import os

    # Initialize Spark session
    spark = (
        SparkSession.builder
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")

    print("--- SPARK SESSION STARTED SUCCESSFULLY ---", flush=True)

    # Get Kafka details from environment variables
    bootstrap_servers = os.environ.get("KAFKA_BOOTSTRAP_SERVERS")
    topic = os.environ.get("KAFKA_TOPIC")

    hdfs_output_path = "/user/stackable/test-output"

    print(f"Reading from Kafka topic '{topic}' at {bootstrap_servers}", flush=True)
    print(f"Writing to HDFS path '{hdfs_output_path}'", flush=True)

    # Read a stream from Kafka
    df = (
        spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", bootstrap_servers)
        .option("subscribe", topic)
        .load()
    )

    # Perform a simple transformation (uppercase)
    transformed_df = df.select(
        col("key").cast("string"),
        col("timestamp"),
        upper(col("value").cast("string")).alias("value")
    )

    # 1. SINK TO CONSOLE (FOR DEBUGGING)
    console_query = (
        transformed_df.writeStream
        .outputMode("append")
        .format("console")
        .option("truncate", "false")
        .start()
    )

    # 2. SINK TO HDFS
    hdfs_query = (
        transformed_df.select("value")
        .writeStream
        .outputMode("append")
        .format("text")
        .option("path", hdfs_output_path)
        .option("checkpointLocation", f"{hdfs_output_path}/_checkpoint")
        .start()
    )

    print(f"--- WAITING FOR KAFKA MESSAGES, WRITING TO CONSOLE AND {hdfs_output_path} ---", flush=True)

    hdfs_query.awaitTermination()
