apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-spark-configmap
data:
  process_reviews.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, from_json, avg, count, sum
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType

    # Truststore is created by initContainer
    truststore = "/truststore/truststore.p12"
    if not os.path.exists(truststore):
        print(f"ERROR: Truststore not found at {truststore}")
        truststore = None
    else:
        print(f"Using truststore at {truststore}")

    spark = SparkSession.builder.appName("SteamReviews").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    bootstrap = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "simple-kafka-broker-default-bootstrap.default.svc.cluster.local:9093")
    topic = "game_comments"
    security_protocol = os.environ.get("KAFKA_SECURITY_PROTOCOL", "SSL")

    schema = StructType([
        StructField("app_id", IntegerType(), True),
        StructField("review_id", IntegerType(), True),
        StructField("author", StringType(), True),
        StructField("language", StringType(), True),
        StructField("recommended", BooleanType(), True),
        StructField("steam_purchase", BooleanType(), True),
        StructField("votes_up", IntegerType(), True),
        StructField("weighted_vote_score", FloatType(), True),
        StructField("timestamp_unix", IntegerType(), True),
        StructField("timestamp", StringType(), True),
        StructField("review", StringType(), True),
        StructField("votes_funny", IntegerType(), True)
    ])
    
    # Build Kafka reader with SSL
    reader = (spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", bootstrap)
        .option("subscribe", topic)
        .option("startingOffsets", "earliest")
        .option("kafka.security.protocol", security_protocol)
        .option("kafka.ssl.endpoint.identification.algorithm", ""))
    
    if truststore and os.path.exists(truststore):
        reader = reader.option("kafka.ssl.truststore.location", truststore)
        reader = reader.option("kafka.ssl.truststore.type", "PKCS12")
        reader = reader.option("kafka.ssl.truststore.password", "changeit")
    
    raw_df = reader.load()
    parsed_df = raw_df.select(from_json(col("value").cast("string"), schema).alias("data"), col("timestamp").alias("ingest_time")).select("data.*", "ingest_time")

    analytics_df = parsed_df.groupBy("recommended").agg(
        count("review_id").alias("total_reviews"),
        avg("weighted_vote_score").alias("avg_quality_score"),
        sum("votes_up").alias("total_helpful_votes")
    )

    query_cold = parsed_df.writeStream.format("parquet").option("path", "/user/stackable/archive/reviews").option("checkpointLocation", "/user/stackable/checkpoints/reviews_cold").outputMode("append").start()
    query_hot = analytics_df.writeStream.format("mongodb").option("checkpointLocation", "/user/stackable/checkpoints/reviews_hot").outputMode("complete").start()

    spark.streams.awaitAnyTermination()

  process_charts.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, from_json, max, min, avg, count
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType

    # Truststore is created by initContainer
    truststore = "/truststore/truststore.p12"
    if not os.path.exists(truststore):
        print(f"ERROR: Truststore not found at {truststore}")
        truststore = None
    else:
        print(f"Using truststore at {truststore}")

    spark = SparkSession.builder.appName("SteamCharts").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    bootstrap = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "simple-kafka-broker-default-bootstrap.default.svc.cluster.local:9093")
    topic = "game_info"
    security_protocol = os.environ.get("KAFKA_SECURITY_PROTOCOL", "SSL")

    schema = StructType([
        StructField("name", StringType(), True),
        StructField("appid", IntegerType(), True),
        StructField("type", StringType(), True),
        StructField("short_description", StringType(), True),
        StructField("developers", StringType(), True),
        StructField("publishers", StringType(), True),
        StructField("genres", StringType(), True),
        StructField("price_overview", StringType(), True),
        StructField("platforms", StringType(), True),
        StructField("header_image", StringType(), True),
        StructField("release_date", StringType(), True),
        StructField("recommendations", StringType(), True),
        StructField("achievements", StringType(), True),
        StructField("screenshots_count", IntegerType(), True),
        StructField("movies_count", IntegerType(), True),
        StructField("timestamp", StringType(), True)
    ])

    # Build Kafka reader with SSL
    reader = (spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", bootstrap)
        .option("subscribe", topic)
        .option("startingOffsets", "earliest")
        .option("kafka.security.protocol", security_protocol)
        .option("kafka.ssl.endpoint.identification.algorithm", ""))
    
    if truststore and os.path.exists(truststore):
        reader = reader.option("kafka.ssl.truststore.location", truststore)
        reader = reader.option("kafka.ssl.truststore.type", "PKCS12")
        reader = reader.option("kafka.ssl.truststore.password", "changeit")
    
    raw_df = reader.load()
    parsed_df = raw_df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

    analytics_df = parsed_df.groupBy("type").agg(
        count("appid").alias("total_games"),
        count("name").alias("games_with_names")
    )

    query_cold = parsed_df.writeStream.format("parquet").option("path", "/user/stackable/archive/charts").option("checkpointLocation", "/user/stackable/checkpoints/charts_cold").outputMode("append").start()
    query_hot = analytics_df.writeStream.format("mongodb").option("checkpointLocation", "/user/stackable/checkpoints/charts_hot").outputMode("complete").start()

    spark.streams.awaitAnyTermination()