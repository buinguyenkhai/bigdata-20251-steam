apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-spark-configmap
data:
  process_reviews.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, from_json, avg, count, sum
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType

    spark = SparkSession.builder.appName("SteamReviews").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    bootstrap = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "simple-kafka-broker-default-bootstrap.default.svc.cluster.local:9092")
    topic = "steam-reviews"

    # Schema for steam_reviews_*.csv
    schema = StructType([
        StructField("review_id", IntegerType(), True),
        StructField("author", StringType(), True),
        StructField("language", StringType(), True),
        StructField("recommended", BooleanType(), True),
        StructField("steam_purchase", BooleanType(), True),
        StructField("votes_up", IntegerType(), True),
        StructField("weighted_vote_score", FloatType(), True),
        StructField("timestamp", StringType(), True),
        StructField("review", StringType(), True)
    ])
    
    # Read Kafka
    raw_df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", bootstrap).option("subscribe", topic).option("startingOffsets", "earliest").load()
    parsed_df = raw_df.select(from_json(col("value").cast("string"), schema).alias("data"), col("timestamp").alias("ingest_time")).select("data.*", "ingest_time")

    # Simple sentiment analysis
    analytics_df = parsed_df.groupBy("recommended").agg(
        count("review_id").alias("total_reviews"),
        avg("weighted_vote_score").alias("avg_quality_score"),
        sum("votes_up").alias("total_helpful_votes")
    )

    # Cold sink(HDFS, for archiving purposes)
    query_cold = parsed_df.writeStream.format("parquet").option("path", "/user/stackable/archive/reviews").option("checkpointLocation", "/user/stackable/checkpoints/reviews_cold").outputMode("append").start()

    # Hot sink(MongoDB, for aggregated analytics, source of dashboard)
    query_hot = analytics_df.writeStream.format("mongodb").option("checkpointLocation", "/user/stackable/checkpoints/reviews_hot").outputMode("complete").start()

    spark.streams.awaitAnyTermination()

  process_charts.py: |
    import os
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, from_json, max, min, avg, count
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType

    spark = SparkSession.builder.appName("SteamCharts").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    bootstrap = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "simple-kafka-broker-default-bootstrap.default.svc.cluster.local:9092")
    topic = "steam-charts"

    # Schema matches steamcharts_*.csv
    schema = StructType([
        StructField("appid", IntegerType(), True),
        StructField("timestamp", StringType(), True),
        StructField("player_count", IntegerType(), True)
    ])

    # Read Kafka
    raw_df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", bootstrap).option("subscribe", topic).option("startingOffsets", "earliest").load()
    parsed_df = raw_df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

    # Simple player traneds analysis
    analytics_df = parsed_df.groupBy("appid").agg(
        max("player_count").alias("peak_playerbase_hourly"),
        min("player_count").alias("lowest_playerbase_hourly"),
        avg("player_count").alias("avg_playerbase_hourly"),
        count("timestamp").alias("hours_tracked")
    )

    # Cold sink(HDFS)
    query_cold = parsed_df.writeStream.format("parquet").option("path", "/user/stackable/archive/charts").option("checkpointLocation", "/user/stackable/checkpoints/charts_cold").outputMode("append").start()

    # Hot sink(MongoDB)
    query_hot = analytics_df.writeStream.format("mongodb").option("checkpointLocation", "/user/stackable/checkpoints/charts_hot").outputMode("complete").start()

    spark.streams.awaitAnyTermination()